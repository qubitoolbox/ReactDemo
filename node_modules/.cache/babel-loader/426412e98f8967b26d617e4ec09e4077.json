{"ast":null,"code":"var _jsxFileName = \"C:\\\\NewReactResume\\\\resm\\\\src\\\\Components\\\\FirstML\\\\Toxic.js\";\nimport * as toxicity from '@tensorflow-models/toxicity'; // The minimum prediction confidence.\n\nconst threshold = 0.9; // Load the model. Users optionally pass in a threshold and an array of\n// labels to include.\n\nclass merC extends React.Component {}\n\nexport default function Toxicity1() {\n  toxicity.load(threshold).then(model => {\n    const sentences = ['you suck'];\n    model.classify(sentences).then(predictions => {\n      // `predictions` is an array of objects, one for each prediction head,\n      // that contains the raw probabilities for each input along with the\n      // final prediction in `match` (either `true` or `false`).\n      // If neither prediction exceeds the threshold, `match` is `null`.\n      console.log(predictions);\n      /*\r\n      prints:\r\n      {\r\n      \"label\": \"identity_attack\",\r\n      \"results\": [{\r\n          \"probabilities\": [0.9659664034843445, 0.03403361141681671],\r\n          \"match\": false\r\n      }]\r\n      },\r\n      {\r\n      \"label\": \"insult\",\r\n      \"results\": [{\r\n          \"probabilities\": [0.08124706149101257, 0.9187529683113098],\r\n          \"match\": true\r\n      }]\r\n      },\r\n      ...\r\n      */\n    });\n  });\n  return React.createElement(\"div\", {\n    __source: {\n      fileName: _jsxFileName,\n      lineNumber: 46\n    },\n    __self: this\n  }, threshold);\n}","map":{"version":3,"sources":["C:/NewReactResume/resm/src/Components/FirstML/Toxic.js"],"names":["toxicity","threshold","merC","React","Component","Toxicity1","load","then","model","sentences","classify","predictions","console","log"],"mappings":";AAAA,OAAO,KAAKA,QAAZ,MAA0B,6BAA1B,C,CACA;;AACA,MAAMC,SAAS,GAAG,GAAlB,C,CAEA;AACA;;AACA,MAAMC,IAAN,SAAmBC,KAAK,CAACC,SAAzB,CAAkC;;AAIlC,eAAe,SAASC,SAAT,GAAqB;AAEhCL,EAAAA,QAAQ,CAACM,IAAT,CAAcL,SAAd,EAAyBM,IAAzB,CAA8BC,KAAK,IAAI;AAEvC,UAAMC,SAAS,GAAG,CAAC,UAAD,CAAlB;AAEAD,IAAAA,KAAK,CAACE,QAAN,CAAeD,SAAf,EAA0BF,IAA1B,CAA+BI,WAAW,IAAI;AAC1C;AACA;AACA;AACA;AAEAC,MAAAA,OAAO,CAACC,GAAR,CAAYF,WAAZ;AACA;;;;;;;;;;;;;;;;;;AAkBH,KAzBD;AA0BC,GA9BD;AAgCA,SACA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,KACKV,SADL,CADA;AAMH","sourcesContent":["import * as toxicity from '@tensorflow-models/toxicity';\r\n// The minimum prediction confidence.\r\nconst threshold = 0.9;\r\n\r\n// Load the model. Users optionally pass in a threshold and an array of\r\n// labels to include.\r\nclass merC extends React.Component{\r\n    \r\n}\r\n\r\nexport default function Toxicity1() {\r\n\r\n    toxicity.load(threshold).then(model => {\r\n    \r\n    const sentences = ['you suck'];\r\n\r\n    model.classify(sentences).then(predictions => {\r\n        // `predictions` is an array of objects, one for each prediction head,\r\n        // that contains the raw probabilities for each input along with the\r\n        // final prediction in `match` (either `true` or `false`).\r\n        // If neither prediction exceeds the threshold, `match` is `null`.\r\n\r\n        console.log(predictions);\r\n        /*\r\n        prints:\r\n        {\r\n        \"label\": \"identity_attack\",\r\n        \"results\": [{\r\n            \"probabilities\": [0.9659664034843445, 0.03403361141681671],\r\n            \"match\": false\r\n        }]\r\n        },\r\n        {\r\n        \"label\": \"insult\",\r\n        \"results\": [{\r\n            \"probabilities\": [0.08124706149101257, 0.9187529683113098],\r\n            \"match\": true\r\n        }]\r\n        },\r\n        ...\r\n        */\r\n    });\r\n    });\r\n\r\n    return(\r\n    <div>\r\n        {threshold}\r\n    </div>\r\n    );\r\n\r\n}"]},"metadata":{},"sourceType":"module"}